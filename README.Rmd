---
output: dynbenchmark::github_markdown_nested
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = FALSE, echo = FALSE)
```

```{r}
library(tidyverse)
library(dynbenchmark)
```

[![Build Status](https://api.travis-ci.org/dynverse/dynbenchmark.svg)](https://travis-ci.org/dynverse/dynbenchmark)
![Lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg) <a href = "package/man/figures/logo.svg"><img src="package/man/figures/logo.png" align="right" /></a>

# Benchmarking trajectory inference methods

This repo contains the scripts to reproduce the manuscript

A comparison of single-cell trajectory inference methods: towards more accurate and robust tools  
Wouter Saelens\*, Robrecht Cannoodt\*, Helena Todorov, Yvan Saeys  
bioRxiv 276907; doi: https://doi.org/10.1101/276907

## Experiments

From start to finish, the repository is divided into several experiments, each with their own scripts and results. These are accompanied by documentation using github READMEs and can be easily explored by going to the appropriate folders:

```{r results = 'asis'}
extract_scripts_documentation("scripts", recursive = FALSE) %>% 
  mutate(
    scripts = location,
    results = map_chr(scripts, dynbenchmark::link_to_results)
  ) %>% 
  mutate(
    ix = case_when(is.na(ix) ~ "", TRUE ~ as.character(ix)),
    id = label_long(id),
    scripts = paste0("[", id, " scripts](", scripts, ")"),
    results = case_when(is.na(results) ~ "", TRUE ~ paste0("[", id, " results](", results, ")"))
  ) %>% 
  select(`\\#` = ix, id, scripts, results) %>% 
  knitr::kable()
```

We also have several additional subfolders:

* manuscript: source files for producing the manuscript
* package: an R package with several helper functions
* data: raw data files required by the scripts
* derived: intermediate data files produced by the scripts

## Datasets

The benchmarking pipeline generates (and uses) the following datasets:

* **Gold standard single-cell datasets**, both real and synthetic, used to evaluated the trajectory inference methods [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1211533.svg)](https://doi.org/10.5281/zenodo.1211533)

```{r, echo = FALSE, message=FALSE}
set.seed(1)

datasets <- list_datasets() %>% group_by(source) %>% sample_n(1) %>% pull(id) %>% load_datasets()

datasets$plot_dimred <- mapdf(datasets, ~plot_dimred(., dimred = dyndimred::dimred_landmark_mds) + ggtitle(.$source) + theme(plot.title = element_text(size = 10)))
p <- patchwork::wrap_plots(datasets$plot_dimred, ncol = nrow(datasets))
ggplot2::ggsave("package/man/figures/datasets.png", width = nrow(datasets)*3, height = 1*3)
```

![datasets](package/man/figures/datasets.png)

* **The performance of methods** used in [dynguidelines](https://www.github.com/dynverse/dynguidelines). _Not yet available_

* **General information about trajectory inference methods**, available as a data frame in `dynmethods::methods`

## Benchmarking your own method

Explanation coming soon. Feel free to make an issue or send us an e-mail if you want your method to be included.

## Guidelines

Based on the results of the benchmark, we provide context-dependent user guidelines. If you want to run methods based on these guidelines, see the [dynorepository](https://github.com/dynverse/dyno). For the guidelines itself, see the [dynguidelines ](https://github.com/dynverse/dynguidelines).
