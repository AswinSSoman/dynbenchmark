---
output: github_document
---

# Metric conformity

```{r include=FALSE}
library(dynbenchmark)
library(tidyverse)

knitr::opts_chunk$set(echo = FALSE)
```

Differences between two datasets should be reflected in certain changes in the metrics. This can be formalised in a set of rules, for example:

* If the position of some cells are different than in the gold standard, the score should decrease.
* If the topology of the network is different than that in the gold standard, the score should not be perfect.
* The more cells are filtered from the trajectory, the more the score should decrease.

Here, we assess whether metrics conforms such rules empirically:

```{r, results = "asis"}
extract_scripts_documentation(".", recursive = FALSE) %>% 
  arrange(ix) %>% 
    mutate(
    script = glue::glue("[`{id}`]({file})"),
    order = ifelse(is.na(ix), "", ix)
  ) %>%
  select(order, script, description = title) %>%
  knitr::kable()
```

