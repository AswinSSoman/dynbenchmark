---
title: "A comparison of single-cell trajectory inference methods: towards more accurate and robust tools"
output: dynbenchmark::pdf_manuscript
---


```{r setup, include=FALSE}
library(dynbenchmark)
library(tidyverse)


tools <- read_rds(result_file("tools.rds", "03-methods"))
methods <- read_rds(result_file("methods.rds", "03-methods"))
```


Wouter Saelens* ¹ ², Robrecht Cannoodt* ¹ ² ³, Helena Todorov¹ ² ⁴, Yvan Saeys¹ ²


\* Equal contribution  
¹ Data mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium.  
² Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium.  
³ Center for Medical Genetics, Ghent University Hospital, Ghent, Belgium.  
⁴ Centre International de Recherche en Infectiologie, Inserm, U1111, Université Claude Bernard Lyon 1, CNRS, UMR5308, École Normale Supérieure de Lyon, Univ Lyon, F-69007, Lyon, France


*`r format(Sys.time(), '%d %B, %Y')`*


`dynverse/dynbenchmark/commit/`r git2r::revparse_single(".","HEAD")$sha``
```{r}
n_tools <- nrow(tools)
n_methods_evaluated <- nrow(filter(methods, evaluated, source %in% c("tool", "adaptation", "offtheshelf")))
n_control_methods_evaluated <- nrow(filter(methods, evaluated, source %in% c("control")))
n_datasets_real <- nrow(list_datasets("real"))
n_datasets_synthetic <- nrow(list_datasets("synthetic"))
```
# Abstract


Recent technological advances allow unbiased investigation of cellular dynamic processes, by generating -omics profiles of thousands of single cells and computationally ordering these cells along a trajectory. Since 2014, at least →`r n_tools`← tools for trajectory inference have been developed, but due to high variability in their inputs and outputs, these methods are difficult to compare. As a result, a comprehensive assessment of the performance of trajectory inference methods is still lacking, and more importantly also guidelines for new users to the field. 


In this study, we evaluated a total of →`r n_methods_evaluated`← methods in terms of cellular →ordering, topology, scalability, and user friendliness←. Our results indicate that existing methods are typically better at ordering the cells rather than inferring the correct trajectory topology, and that there is no "one-fits-all" method. Instead, the choice of method →mostly← depends on the trajectory topology present in the data. →To aid users with selecting the most appropriate method, we therefore developed a set of guidelines, available as an interactive app at `r print_url("http://dynverse.duckdns.org")`.← While the field certainly has matured significantly since its inception, further progress is still necessary in order to cope with increasingly complex and novel use cases. We hope to spearhead such developments and the field as a whole, by making our evaluation pipeline easily extensible and freely available at `r print_url("https://www.github.com/dynverse/dynverse")`.


# Introduction
Single-cell -omics technologies now make it possible to model biological systems more accurately than ever before [@tanay_scaling_2017]. One area where single-cell data has been particularly useful is in the study of cellular dynamic processes, such as the cell cycle, cell differentiation and cell activation [@etzrodt_quantitativesinglecellapproaches_2014]. Such dynamic processes can be modelled computationally using trajectory inference (TI) methods, which order cells along a trajectory by their overall similarity in -omics datasets, mainly transcriptomics [@trapnell_definingcelltypes_2015; @cannoodt_computational_2016; @moon_manifoldlearningbasedmethods_2018]. The resulting trajectories are most often linear, bifurcating or tree-shaped, but more recent methods also allow to identify more complex trajectory topologies such as cyclic [@liu_reconstructingcellcycle_2017] or disconnected [@wolf_graphabstractionreconciles_2017] graphs. TI methods offer an unbiased and transcriptome-wide understanding of a dynamic process [@tanay_scaling_2017],  thereby allowing the objective identification of new (primed) subsets of cells [@schlitzer_identificationcdc1cdc2committed_2015], delineation of a differentiation tree [@velten_humanhaematopoieticstem_2017; @see_mappinghumandc_2017] and inference of regulatory interactions responsible for one or more bifurcations [@aibar_scenicsinglecellregulatory_2017]. 
Current applications of TI focus on specific subsets of cells, but ongoing efforts to construct transcriptomic catalogues of whole organisms [@regev_scienceforumhuman_2017; @han_mappingmousecell_2018] underline the urgency for accurate, scalable[@aibar_scenicsinglecellregulatory_2017; @angerer_singlecellsmake_2017] and user-friendly TI methods.




<!-- §2: Plethora of new TI methods -->
A plethora of TI methods has been developed over the last years, and even more are being created every month (`r ref("stable", "tools")`). →Indeed, in several repositories listing single-cell tools,  `r print_url("www.omictools.org")` [@henry_omictoolsinformativedirectory_2014], the "awesome-single-cell" list (`r print_url("https://github.com/seandavi/awesome-single-cell")`) [@davis_awesomesinglecelllistsoftware_2018] and `r print_url("www.scRNA-tools.org")` [@zappia_exploringsinglecellrnaseq_2017], TI methods are one the largest categories.← All methods have their own unique set of characteristics in terms of underlying algorithm, the prior information they require and the outputs they produce. Two of the most distinctive differences between TI methods are whether they fix the topology of the trajectory or not, and what type(s) of graph topologies they can detect. Early TI methods typically fixed the topology algorithmically (e.g. linear [@bendall_singlecelltrajectorydetection_2014; @schlitzer_identificationcdc1cdc2committed_2015; @shin_singlecellrnaseqwaterfall_2015; @campbell_bayesiangaussianprocess_2015] or bifurcating [@haghverdi_diffusionpseudotimerobustly_2016; @setty_wishboneidentifiesbifurcating_2016]), or through parameters provided by the user [@trapnell_dynamics_2014; @matsumoto_scoupprobabilisticmodel_2016]. These methods therefore mainly focused on correctly ordering the cells along the fixed topology. →Most recent methods also infer the topology computationally, which increases the difficulty of the problem at hand, but allows an unbiased identification of both the ordering inside a branch and the topology connecting these branches.←




```{r, eval = FALSE}
add_stable(
  read_rds(result_file("tools_table.rds", "03-methods"))[[params$table_format]],
"tools",
paste0("Overview of the trajectory inference methods included in this study. ", glue::glue_data(non_inclusion_reasons, "{footnote}: {long}") %>% glue::glue_collapse("; "))
)
```




<!-- §3: Problem statement -->
Given the diversity in TI methods, an important issue to address is a quantitative assessment of their performance and robustness. Many attempts at tackling this issue have already been made [@haghverdi_diffusionpseudotimerobustly_2016; @ji_tscanpseudotimereconstruction_2016; @welch_slicerinferringbranched_2016; @matsumoto_scoupprobabilisticmodel_2016; @duverle_celltreebioconductorpackage_2016; @cannoodt_scorpiusimprovestrajectory_2016; @lonnberg_singlecellrnaseqcomputational_2017; @campbell_probabilisticmodelingbifurcations_2017], but a comprehensive benchmarking evaluation of TI methods, which compares a large set of methods on a large set of datasets, is still lacking. This is problematic, as new users to the field are confronted with an overwhelming choice of TI methods, without a clear idea about which method would optimally solve their problem. Moreover, the strengths and weaknesses of existing methods need to be assessed, so that new developments in the field can focus on improving the current state-of-the-art.


<!-- §4: Our study -->
In this study, we performed a comprehensive evaluation of  →`r n_methods_evaluated`← TI methods (`r ref("fig", "ti_evaluation_overview", "a")`, `r ref("sfig", "overview_evaluation_expanded")`). The inclusion criterion for TI methods was primarily based on their free availability and presence of a programming interface (`r ref("stable", "tools")`). →We evaluated each methods on four core aspects: (i) their accuracy given a gold or silver standard on `r n_datasets_real` real and `r n_datasets_synthetic` synthetic datasets, (ii) the user friendliness of the provided software and documentation, (iii) their scalability with both the number of cells and features, and (iv) the stability of the predictions on the same dataset. Finally, we combined these metrics to provide a set of practical guidelines for users. ←


```{r}
add_fig(
  fig_path = raw_file("overview_evaluation_v6.svg", ""),
  ref_id = "ti_evaluation_overview", 
  caption_main = "Overview of several key aspects of the evaluation.", 
  caption_text = "a) A schematic overview of our evaluation pipeline. b) In order to make the trajectories comparable to each other, a common trajectory model was used to represent gold standard trajectories from the real and synthetic datasets, as well as any predicted trajectories from TI methods. c) Trajectories are automatically classified into one of nine trajectory types, with increasing complexity. d) →We defined four metrics each assessing the similarity of a different aspect of the trajectory: the topology, the branch assignment, the cell positions and the important features.←",
   width = 13,
  height = 8, 
)
```


```{r}
add_sfig(
  fig_path = raw_file("overview_evaluation_expanded_v1.svg", ""),
  ref_id = "overview_evaluation_expanded",
  caption_main = "Extended overview of our evaluation pipeline.", 
  caption_text = "From literature we extracted a quality control checklist, a list of trajectory inference methods, a set of real datasets containing a trajectory and real regulatory networks used to generate the synthetic data. We created a wrapper of each method, so that its output was transformed into a common probabilistic trajectory model, and used these to infer trajectories on all real and synthetic datasets using their default parameters. Using several similarity metrics, we compared the gold standard of the real and synthetic datasets with the inferred trajectories. We also evaluated the quality of each method using the quality control checklist, and used both the benchmark results and quality control results to produce a final set of guidelines for method's users.", 
  width = 15, 
  height = 7.5
)
```


# Results


## Evaluation of trajectory inference methods


<!-- Evaluation overview -->
In order to make gold standard trajectories and predicted trajectories directly comparable to each other, we developed a common probabilistic model for representing trajectories from all possible sources (`r ref("fig", "ti_evaluation_overview", "b")`). In this model, the overall topology is represented by a network of "milestones", and the cells are placed within the space formed by each set of connected milestones. The milestone network is automatically classified into one of the nine predefined trajectory types (`r ref("fig", "ti_evaluation_overview", "c")`), which go from the very basic topologies (linear, cyclical and bifurcating) to the most complex (cyclic graphs and disconnected graphs). →We defined several possible metrics to compare a prediction to the reference trajectory (`r ref("snote", "metrics")`). Based on an analysis of their robustness and conformity to a set of rules (`r ref("snote", "metrics")`), we chose 4 metrics each assessing a different aspect of a trajectory: the cell ordering (`r label_metric("correlation")`), the topology (`r label_metric("him")`), the quality of differentially expressed features  (`r label_metric("featureimp_cor")`), and the quality of clustering of the cells into branches  (`r label_metric("F1_branches")`) (`r ref("fig", "ti_evaluation_overview", "d")`).←The data compendium consisted of both synthetic datasets, which offer the most exact gold standard, and real datasets, which offer the highest biological relevance. These real datasets come from a variety of single-cell technologies, organisms, and dynamic processes, and contain several types of trajectory topologies (`r ref("stable", "datasets")`). →For synthetic datasets we used several data simulators, including our own which simulates gene regulatory networks using a thermodynamic model of gene regulation [@schaffter_genenetweaversilicobenchmark_2011]. We paired each simulated datasets with a real dataset to match its dimensions, number of differentially expressed genes, drop-out rates and other statistical properties.←


<!-- Overall evaluation results -->
```{r echo=FALSE, results="asis"}
add_fig(
  fig_path = result_file("overview.pdf", experiment_id = "08-summary"),
  ref_id = "results_overview", 
  caption_main = glue::glue("Overview of the results on the evaluation of {n_methods_evaluated} TI methods and {n_control_methods_evaluated} control methods."), 
  caption_text = "a) The methods were characterised according to the most complex trajectory type they can infer, whether or not the inferred topology is constrained by the algorithm or a parameter, and which prior information a method requires or can optionally use. b) The methods are ordered according to the overall score achieved both in the benchmark and QC evaluation. `r {n_control_methods_evaluated}` control methods were also included as an indication of baseline performance c) Also shown are the aggregated scores per metric, source and trajectory type, as well as the average execution time across all datasets and the percentage of executions in which no output was produced. d) Performance in the quality control evaluation is highly variable, even amongst the highest ranked methods according to the benchmark evaluation. Also listed are the quality control scores aggregated according to practicality and the different categories.",
  width = 15,
  height = 20
)
```


<!-- Evaluation robustness -->






## Relationships between method characteristics and performance


<!-- Inverse relation between edge flip and correlation -->


<!-- Topological complexity versus performance -->


## Method stability and scalability
## Method quality control






# Discussion


<!-- The context -->
In this study, we presented a large scale evaluation of the performance and code of  `r n_methods_evaluated`  TI methods. By using a common structure and three metrics to compare the methods’ outputs, we were able to assess the performance of the methods on more than a hundred datasets, which allowed us to highlight the strengths and weaknesses of each method on different trajectory types. We also assessed the code quality of these methods, to provide information on important scientific and software development practices for these methods.


<!-- FOR METHOD USERS -->
<!-- Practical guidelines -->
Based on the results of our benchmark, we propose a set of practical guidelines for method users (`r ref("fig", "user_guidelines")`). We postulate that, as a method's performance is heavily dependent on the trajectory type being studied, the choice of method should be primarily driven by the prior knowledge of the user about which trajectory topology is expected in the data. For the majority of use cases, the user will know very little about the expected trajectory, except perhaps whether the data is expected to contain multiple disconnected trajectories, cycles or a complex tree structure. In each of these use cases, a different set of methods performed optimally, with Monocle DDRTree and AGA Pseudotime performing best when the data contained a complex tree, Slingshot and TSCAN performing well on less complex trajectory structures with at most one bifurcation, and AGA Pseudotime and AGA performing well when the data can contain a cycle or multiple disconnected trajectories. In the case where the user expects a particular topology, our evaluation suggests the use of Angle (one of our control methods) and reCAT for cycles, SCORPIUS and cellTree for linear trajectories, TSCAN and Slingshot for bifurcating trajectories, and Slingshot and SCOUP for multifurcating trajectories, although certain methods (those with a free topology) could return other topologies if it would fit the data more accurately (`r ref("fig", "user_guidelines")`).


```{r echo=FALSE, results="asis"}
add_fig(
  fig_path = ggplot(),#figure_file("tree.svg", experiment_id = "7-user_guidelines"),
  ref_id = "user_guidelines", 
  caption_main = "Practical guidelines for method users.", 
  caption_text = "As the performance of a method most heavily depends on the topology of the trajectory, the choice of TI method will be primarily influenced by the user's existing knowledge about the expected topology in the data. We therefore devised a set of practical guidelines, which combines the method's performance, user friendliness and the number of assumptions a user is willing to make about the topology of the trajectory. Methods to the right are ranked according to their performance on a particular (set of) trajectory type. Further to the right are shown the user friendliness scores (++: ≥ 0.95, +: ≥ 0.85, ± ≥ 0.75, - ≥ 0.65), overall performance (++: top method, +: difference between top method's performance ≥ -0.05, ±: ≥-0.2, -: ≥ -0.5) and required prior information."
)
```


<!-- Further considerations -->
When choosing a method, it is important to take two further points into account. First, it is critical that a trajectory, and the downstream results and/or hypotheses originating from it, are confirmed by multiple TI methods. This is to make sure the prediction is not biased due to the given parameter setting or the particular algorithms underlying a TI method. Second, even if the expected topology is known, it can be beneficial to also try out methods which make less assumptions about the trajectory topology. When the expected topology is confirmed using such a method, it provides extra evidence to the user's knowledge of the biological system. When a more complex topology is produced, this could indicate the presence of a more complex trajectory in the data than was expected by the user.


<!-- A common trajectory analysis framework -->
Critical to the broad applicability of TI methods is standardisation of the input and output interfaces of TI methods, so that users can effortlessly execute TI methods on their dataset of interest, compare different predicted trajectories, and apply downstream analyses such as network inference [@aibar_scenicsinglecellregulatory_2017] or module detection [@saelens_comprehensiveevaluationmodule_2018]. Our framework is an initial attempt at tackling this problem, and we illustrate its usefulness here by comparing the predicted trajectories of several top-performing methods on a randomly picked dataset (`r ref("fig", "example_predictions")`). In the future, this framework should be extended to allow easier downstream analyses and also to allow easier interpretation by overlaying custom values such as cell type or expression values. In addition, further discussion within the field is required in order to arrive at a consensus concerning a common interface for trajectory models, which can include additional features such as uncertainty and gene importance.


```{r echo=FALSE, results="asis"}
add_fig(
  fig_path = ggplot(),#figure_file("bifurcating_example.svg", experiment_id = "14-example_predictions"),
  ref_id = "example_predictions", 
  caption_main = "Demonstration of how a common framework for TI methods facilitates broad applicability using an example dataset.", 
  caption_text = "a) The dataset consists of fibroblasts being reprogrammed towards neurons and myocytes, and thus contains a bifurcating dynamic process. A multi-dimensional scaling of dataset is shown at the bottom. MEF: mouse embryonic fibroblast b) A common visualisation of the predicted trajectories allows to easily compare the similarities and differences between predictions. The predictions are in line with earlier results in that Slingshot underestimates the complexity of the topology, AGA Pseudotime and Monocle DDRTree overestimate the complexity, and TSCAN is able to correctly identify the trajectory topology. c) More insight can be gained into the ordering of the cells by placing the branches one after the other. The predicted topology is shown in the bottom of each plot. d) Method-specific visualisations provide insight into the inner workings of a method's methodology and into how the trajectory was constructed. e) Our metrics can be used to compare the output to a gold standard or to each other. In this case, while only TSCAN and cellTree correctly predict the topology, the ordering of Slingshot and AGA Pseudotime on this dataset corresponds better to the expected ordering."
)
```


<!-- FOR METHOD DEVELOPERS -->
<!-- New tools: challenges and possibilities -->
Our study indicates that the field of trajectory inference is maturing, primarily for linear and bifurcating trajectories. Nonetheless, we also highlight several ongoing challenges, which should be addressed before TI can be a reliable and fast tool for analysing single-cell -omics datasets with complex trajectories. Foremost, new methods should focus on improving the inference of the topology for tree and more complex topologies, and should include better ordering methods, perhaps borrowed from the top linear TI methods. Furthermore, new tools should incorporate higher standards for code assurance and documentation, to improve the friendliness towards both users and developers. To support the development of these new tools, we provide all datasets from this study [@cannoodt_goldstandardsinglecell_] and provide the code to compare methods in a series of R packages available at [github.com/dynverse/dynverse](https://github.com/dynverse/dynverse).


<!-- The robustness of the evaluation & development of new methods -->
We found that the performance of a method can be very variable between datasets, and therefore included a large set of both real and synthetic data within our evaluation, leading to a robust overall ranking of the different methods. Nonetheless, we also found that it is possible to define for almost every method a relatively large subset of datasets on which this method on average performs the best. These findings raise some important issues, as we want to avoid that a race to be the first on the ranking would stifle creativity, given that there is a high risk that a certain new methodology would not improve upon the state-of-the-art. Indeed, we firmly believe that "good-yet-not-the-best" methods [@norel_selfassessmenttrap_2011] can still provide a very valuable contribution to the field, especially if they make use of novel algorithms, return a more scalable solution, or provide a unique insight in specific use cases. Some examples for the latter include PhenoPath, which can include additional covariates in its model, Ouija, which returns an measure of uncertainty of each cell's position within the trajectory, and StemID, which can infer the directionality of edges within the trajectory. We feel that such methods should also be valued, and encourage their use provided that they are contained in a user friendly tool.


________________
# Online Methods
## Data and code availability
→ 
The real and synthetic datasets used in this study are deposited on Zenodo ([doi.org/10.5281/zenodo.1211533](https://doi.org/10.5281/zenodo.1211533)) [@cannoodt_goldstandardsinglecell_]. 


The code to fully reproduce the results from this study has been split into several R packages and is available on github at `r print_url("github.com/dynverse/dynverse")`. These include packages for wrapping trajectories (`r print_url("github.com/dynverse/dynwrap")`), all trajectory inference methods (`r print_url("github.com/dynverse/dynverse")`), the evaluation metrics (`r print_url("github.com/dynverse/dyneval")`) and the guidelines app packages (`r print_url("github.com/dynverse/dynguidelines")`). The main analysis repository is available at `r print_url("github.com/dynverse/dynbenchmark")` and is divided into several experiments. In this repository, we provide READMEs for both the scripts and the results, which can be easily browsed and explored on the github website. 
←
## Trajectory inference methods
<!-- Overview -->
We gathered a list of `r n_tools` trajectory inference tools (`r ref("stable", "tools")`), by searching the literature for "trajectory inference" and "pseudotemporal ordering", and based on two existing lists found online [@davis_awesomesinglecelllistsoftware_2018; @gitter_singlecellpseudotimeoverviewalgorithms_2018].
We welcome any contributions by creating an issue at →`r print_url("github.com/dynverse/dynmethods/issues")`←.


<!-- Inclusion criterion -->
Methods were excluded from the evaluation based on several criteria: `r glue::glue_collapse(glue::glue("({non_inclusion_reasons$footnote}) {non_inclusion_reasons$long}"), ", ")`. →The discussions on individual methods can be found at `r print_url("https://github.com/dynverse/dynmethods/issues?q=label:unwrappable")`. In the end, we included  `r n_tools` tools in the evaluation, constituting `r n_methods_evaluated` methods.←


<!-- Components -->
The selected methods were dissected into a set of algorithmic components. Each component can have a significant impact on the performance, scalability, and output. Across all methods, these components can be broadly grouped into two stages; (i) conversion to a simplified representation using dimensionality reduction, clustering or graph building and (ii) ordering the cells along the simplified representation [@cannoodt_computational_2016]. Components are frequently shared between different algorithms. For example, minimal spanning trees (MST), used in the first single-cell RNA-seq trajectory inference methods [@trapnell_dynamics_2014], is shared by almost half of the methods we evaluated.
→
### Method wrappers
Each method was wrapped within a docker and singularity container (available at `r print_url("https://github.com/dynverse/dynverse/tree/master/methods")`). These containers are automatically built on both Singularity Hub (`r print_url("https://singularity-hub.org/")`) and Docker Hub (`r print_url("https://hub.docker.com/u/dynverse")`).
For each method, we wrote a wrapper script based on example scripts or tutorials provided by the authors (as mentioned in the respective wrapper scripts). This script reads in the input data (as described below), runs the method, and outputs the files required to construct a trajectory (as described below). We also created a script to generate an example dataset, which is used to automatically test every method using travis continuous integration (`r print_url("https://travis-ci.org/dynverse")`).


We used the github issues system to contact the authors of each method, and asked for feedback on the wrappers, the metadata, and the quality control. About ⅓ of authors responded, and we improved the wrappers based on their feedback. These discussions can be viewed on github: `r print_url('https://github.com/dynverse/dynmethods/issues?q=label:"method+discussion"')`. 
←
### Method input
As input, we provided each method with either the raw count data (after cell and gene filtering) or normalised expression values, based on the description in the method documentation or from the study describing the method.
<!-- Prior information -->
A large portion of the methods requires some form of prior information (e.g. a start cell) in order to be executable. Other methods optionally allow to exploit certain prior information. Prior information can be supplied as a starting cell from which the trajectory will originate, a set of important marker genes, or even a grouping of cells into cell states. Providing prior information to a TI method can be both a blessing and a curse. In one way, prior information can help the method to find the correct trajectory among many, equally likely, alternatives. On the other hand, incorrect or noisy prior information can bias the trajectory towards current knowledge. Moreover, prior information is not always easily available, and its subjectivity can therefore lead to multiple equally plausible solutions, restricting the applicability of such TI methods to well studied systems.


The prior information was extracted from the gold standard as follows:


* **Start cells** The identity of one or more start cells. For both real and synthetic data, a cell was chosen which was the closest (in geodesic distance) to each milestone with only outgoing edges. For ties, one random cell was chosen. For cyclic datasets, a random cell was chosen.
* **End cells** The identity of one or more end cells. Similar as the start cells, but now for every state with only ingoing edges.
* **# end states** Number of terminal states. Number of milestones with only ingoing edges.
* **Grouping** For each cell a label to which state/cluster/branch it belongs. For real data, the states from the gold/silver standard. For synthetic data, each milestone was seen as one group, and cells were assigned to their closest milestone.
* **# branches** Number of branches/intermediate states. For real data, the number of states in the gold/silver standard. For synthetic data, the number of milestones.
→
* **Discrete time course** For each cell a time point from which it was sampled. If available, directly extracted from the gold standard, otherwise the geodesic distance from the root milestone was used. For synthetic data, four discrete timepoints were chosen, at which the cells were “sampled” to provide a time course information reflecting the one provided in real experiments.
* **Continuous time course** For each cell a time point from which it was sampled. For real data this was equal to the discrete time course, for synthetic data we used the internal simulation time of each simulator.
←
### Common trajectory model
Due to the absence of a common format for trajectory models, most methods return a unique set of output formats with few overlaps. →We therefore post-processed the output of each method into a common probabilistic trajectory model← (`r ref("sfig", "overview_wrapping", "a")`). This model consisted of three parts: (i) The milestone network represents the overall network topology, and contains edges between different milestones and the length of the edge between them. (ii) The milestone percentages contains, for each cell, its position between milestones, and sums for each cell to one. (iii) →The regions of delayed commitment, which are connection between three or more milestones. These must be explicitly defined in the trajectory model and per region, one milestone must be directly connected to all other milestones of the region.←


```{r}
#add_sfig(
#  figure_file("overview_wrapping_v1.svg", "manual_figures"),
#  "overview_wrapping", 
#  "A common interface for TI methods.", 
#  "a) The input and output of each TI method is standardised. As input, each TI method #receives either raw or normalised counts, several parameters, and a selection of prior information. After its execution, a method uses one of the seven wrapper functions to transform its output to the common trajectory model. This common model then allows to perform common analysis functions on trajectory models produced by any TI method. b) The specific transformations performed by each of the wrapper functions is shown in more detail.", 
#  15, 
#  7.5
#)
```


Depending on the output of a method, we used different strategies to convert the output to our model (`r ref("sfig", "overview_wrapping", "b")`). Special conversions are denoted by an \*, and will be explained in more detail below.


```{r}
methods_evaluated <- read_rds(result_file("methods_evaluated.rds", "03-methods"))
special_methods <- c("mfa", "dpt", "slice", "sincell", "calista", "urd")
# create functions to list the methods within a particular conversion category
conversion <- function(conversion_oi) {
  methods_evaluated %>% 
    filter(source %in% c("tool", "offtheshelf", "adaptation")) %>%
    filter(map_lgl(output, ~conversion_oi %in% .$outputs)) %>% 
    mutate(name = paste0(name, ifelse(id %in% special_methods, "\\*", ""))) %>% 
    pull(name) %>% 
    sort() %>%
    label_vector()
}
```


* **Type 1, direct:** `r conversion("trajectory")`. The wrapped method directly returned a network of milestones, the regions of delayed commitment, and for each cell is given to what extent it belongs to a milestone. →In some cases, this← indicates that additional transformations were required for the method not covered by any of the following output formats. →Some methods (`r conversion("branch_trajectory")`) returned a branch network instead of a milestone network, and this conversion was done by calculating the line graph of the branch network.←
* **Type 2, linear pseudotime:** `r conversion("linear_trajectory")`. The method returned a pseudotime, which is translated into a linear trajectory where the milestone network contains two milestones and cells are positioned between these two milestones.
* **Type 3, cyclical pseudotime:** `r conversion("cyclic_trajectory")`. The method returned a pseudotime, which is translated into a cyclical trajectory where the milestone network contains three milestones and cells are positioned between these three milestones. →These milestones were positioned at pseudotime 0, ⅓ and ⅔ .←
* **Type 4, end state probability:** `r conversion("end_state_probabilities")`. The method returned a pseudotime and for each cell and end state a probability for how likely a cell will end up in a certain end state. →This was translated into a star-shaped milestone network, with one starting milestone ($M_0$) and several outer milestones ($M_i$), with regions of delayed commitment between all milestones. The milestone percentage of a cell to one of the outer milestones was equal to $\textrm{pseudotime} \times \textrm{Pr}_{M_i}$ , the milestone percentage to the starting milestone was equal to $1-\textrm{pseudotime}$ .←
* **Type 5, cluster assignment:** `r conversion("cluster_graph")`. The method returned a milestone network, and an assignment of each cell to a specific milestone. Cells were positioned onto the milestones they are assigned to→, with milestone percentage equal to 1.←
* **Type 6, project onto nearest branch:** `r conversion("dimred_projection")`. The method returned a milestone network, and a dimensionality reduction of the cells and milestones. The cells were projected onto the closest nearest segment, thus determining the cells position along the milestone network. →If a method also returned a cluster assignment (type 5), we limited the projection of each cell to the closest edge connecting to the milestone of a cell. For these methods, we usually wrote two wrappers, one which included the projection, and one without.←
* **Type 7, cell graph:** `r conversion("cell_graph")`. The method returned a network of cells, 
and which cell-cell transitions are part of the 'backbone' structure. Backbone cells with degree $\neq$ 2 were regarded as milestones, and all other cells were placed on transitions between the milestones. →If methods did not return a distance between pairs of cells, the cells were uniformly positioned between the two milestones. Otherwise, we first calculated the distance between two milestones as the sum of the distances between the cells, and then divided the distance of each pair of cells with the total distance to get the milestone percentages.←


Special conversions were necessary for certain methods:


* **DPT:** We projected the cells onto the cluster network, consisting of a central milestone (this cluster contains the cells which were assigned to the "unknown" branch) and three terminal milestones, each corresponding to a tip point. →This was then processed as a type 1, direct,  method.←
* **Sincell:** To constrain the number of milestones this method creates, we merged two cell clusters iteratively until the percentage of leaf nodes was below a certain cutoff, default at 25%.  →This was then processed as a type 7, cell graph, method.←
* **SLICE:** As discussed in the vignette of SLICE, we ran principal curves one by one for every edge detected by SLICE. →This was then processed as a type 1, direct, method.←
* **mfa:** We used the branch assignment as state probabilities, which together with the global pseudotime were processed →as a type 4, end state probabilities, method.←
→
* **CALISTA:** We assigned the cells to the branch at which the sum of the cluster probabilities of two connected milestones was the highest. The cluster probabilities of the two selected milestones were then used as milestone percentages. This was then processed as a type 1, direct, method.
* **URD:** We extracted the pseudotime of a cell within each branch using the y positions in the tree layout. This was then further processed as a type 1, direct, method.


More information on how each method was wrapped can be found within the comments of each wrapper script, available at `r print_url("https://github.com/dynverse/dynverse/tree/master/methods")`.←


→
### Control methods
We added several control positive and negative control methods: **random**, which generates a random network with 50 milestones according to a Barabási–Albert model, **identity**, which returns the gold standard, and **shuffle**, which will place the cells randomly on the gold standard milestone network.
### Off-the-shelf methods
For baseline performance, we added several "off-the-shelf" TI methods which can be run using just a couple of commands in R.←


* **Component 1:** This method returns the first component of a PCA dimensionality reduction as a linear trajectory. Although conceptually very simple, it has been used in certain studies [@kouno_temporaldynamicstranscriptional_2013; @zeng_pseudotemporalorderingsingle_2017].
* **Angle:** Similar to the previous method, this method computes the angle with respect to the origin in a two-dimensional PCA, and uses this angle as pseudotime for a cyclical trajectory.
→
* **Periodic PrinCurve:** Periodic principal curves, using the R princurve package
* **MST:** Dimensionality reduction with PCA, followed by clustering using the R mclust package, after which the clusters are connected using a minimal spanning tree. The cells are then projected onto the edges of the MST.
←


## Real datasets
We gathered `r sum(list_datasets()$source == "real")` real datasets by searching for "single-cell" at the Gene Expression Omnibus and selecting those datasets in which the cells are sampled from different stages in a dynamic process (`r ref("stable", "datasets")`). The scripts to download and process these datasets are available on our repository (`r print_url("https://github.com/dynverse/dynbenchmark/tree/master/scripts/01-datasets")`). Whenever possible, we preferred to start from the raw counts data. These raw counts were all normalised and filtered using a common pipeline, discussed later.


→For each dataset, we extracted a reference trajectory, consisting of two parts: the cellular grouping (milestones) and the connections between these groups (milestone network). The cellular grouping was provided by the authors of the original study, and we classified it as a gold standard when it was created independently from the expression matrix (such as from cell sorting, the origin of the sample or the time it was sampled) or silver standard otherwise (usually by clustering the expression values). To connect these cell groups, we used the original study to determine the network which the authors validated or otherwise found to be the most likely. In the end, each group of cells was placed on a milestone, having percentage = 1 for that particular milestone. The known connections between these groups were used to construct the milestone network. If there was time data available, we used this as the length of the edge, otherwise we set all the lengths equal to one. ←


```{r}
#add_stable(
#  read_rds(
#     figure_file("datasets.rds","2-dataset_characterisation/2-real")
#  )[[params$table_format]], 
#  "datasets", 
#  "Real datasets used in this study."
#)
```
## Synthetic datasets
→
To generate synthetic datasets, we used four different synthetic data simulators:


* **dyngen:** Simulations of gene regulatory networks, available at `r print_url("https://github.com/dynverse/dyngen")`
* **dyntoy:** Random gradients of expression in the reduced space, available at `r print_url("https://github.com/dynverse/dyntoy")`
* **PROSSTT:** Expression is sampled from a linear model which depends on pseudotime [@papadopoulosPROSSTTProbabilisticSimulation2018]
* **Splatter:** Simulations of non-linear paths between different expression states [@zappia_splattersimulationsinglecell_2017]


For every simulator, we took great care with trying to make the datasets look like real data as  best as possible. To do this, we extracted several parameters from all real datasets. We calculated the number of differentially expressed features within a trajectory using a wilcoxon test between every pair of cell groups. We also calculated several other parameters, such as drop-out rates and library sizes using the Splatter package [@zappia_splattersimulationsinglecell_2017]. These parameters were then given to the simulators when applicable, as described below. To select a diverse set of $k$ reference real datasets, we clustered all the parameters of each real dataset, and used the $k$ cluster centers from a pam clustering (implemented in the R cluster package);
←
### dyngen


```{r}
design_dyngen <- read_rds(result_file("design_dyngen.rds", "01-datasets/02-synthetic"))
```


The dyngen ( `r print_url("https://github.com/dynverse/dyngen")`) workflow to generate synthetic data is based on the well established workflow used in the evaluation of network inference methods [@schaffter_genenetweaversilicobenchmark_2011; @marbach_wisdom_2012] and consists of four main steps: network generation, simulation, gold standard extraction and simulation of the scRNA-seq experiment. At every step, we tried to mirror real regulatory networks, while keeping the model simple and easily extendable. →We simulated a total of `r nrow(design_dyngen)` datasets, with `r length(unique(design_dyngen$modulenet_name))` different topologies.←
#### Network generation


One of the main processes involved in cellular dynamic processes is gene regulation, where regulatory cascades and feedback loops lead to progressive changes in expression and decision making. The exact way a cell choses a certain path during its differentiation is still an active research field, although certain models have already emerged and been tested in vivo. One driver of bifurcation seems to be mutual antagonism, where genes [@xu_regulationbifurcatingcell_2015] strongly repress each other, forcing one of the two to become inactive [@graf_forcingcellschange_2009]. Such mutual antagonism can be modelled and simulated [@wang_quantifyingwaddingtonlandscape_2011; @ferrell_bistabilitybifurcationswaddington_2012]. Although such a two-gene model is simple and elegant, the reality is frequently more complex, with multiple genes (grouped into modules) repressing each other [@yosef_dynamicregulatorynetwork_2013].


→To simulate certain trajectory topologies, we therefore designed module networks in which the cells follow a particular trajectory topology given certain parameters. Two module networks generated linear trajectories (linear and linear long), one generated a bifurcation, one generated a convergence, one generated a multifurcation (trifurcating), two generated a tree (consecutive bifurcating and binary tree), one generated an acyclic graph (bifurcating converging), (bifurcating and converging), one generated a complex fork (trifurcating), one generated a rooted tree (consecutive bifurcating) and two generated simple graph structures (bifurcating loop and bifurcating cycle).←


From these module networks we generated gene regulatory networks in two steps: the main regulatory network was first generated, and extra target genes from real regulatory networks  were added. For each dataset, we used the same number of genes as were differentially expressed in the real datasets. 5% of the genes were assigned to be part of the main regulatory network, and were randomly distributed among all modules (with at least one gene per module). We sampled edges between these individual genes (according to the module network) using a uniform distribution between 1 and the number of possible targets in each module. To add additional target genes to the network, we assigned every regulator from the network to a real regulator in a real network (from regulatory circuits [@marbach_tissuespecificregulatorycircuits_2016]), and extracted for every regulator a local network around it using personalized pagerank (with damping factor set to 0.1), as implemented in the `page_rank` function of the *igraph* package. 


#### Simulation of gene regulatory systems using thermodynamic models


To simulate the gene regulatory network, we used a system of differential equations similar to those used in evaluations of gene regulatory network inference methods [@marbach_wisdom_2012]. In this model, the changes in gene expression ($x_i$) and protein expression ($y_i$) are modeled using ordinary differential equations [@schaffter_genenetweaversilicobenchmark_2011] (ODEs):


$$
\begin{aligned}
\label{eq:mrna_ode}
\frac{dx_i}{dt} &= \underbrace{m \times f(y_1, y_2, ...)}_\text{production} - \underbrace{\lambda \times x_i}_\text{degradation}
\end{aligned}
$$
$$
\begin{aligned}
\label{eq:prot_ode}
\frac{dy_i}{dt} &= \underbrace{r \times x_i}_\text{production} - \underbrace{\Lambda \times y_i}_\text{degradation}
\end{aligned}
$$


where $m$, $\lambda$, $r$ and $\Lambda$ represent production and degradation rates, the ratio of which determines the maximal gene and protein expression. The two types of equations are coupled because the production of protein $y_i$ depends on the amount of gene expression $x_i$, which in turn depends on the amount of other proteins through the activation function $f(y_1, y_2, ...)$.


The activation function is inspired by a thermodynamic model of gene regulation, in which the promoter of a gene can be bound or unbound by a set of transcription factors, each representing a certain state of the promoter. Each state is linked with a relative activation $\alpha_j$, a number between 0 and 1 representing the activity of the promoter at this particular state. The production rate of the gene is calculated by combining the probabilities of the promoter being in each state with the relative activation:


$$
\begin{aligned}
\label{eq:input_function_probabilities}
f(y_1, y_2, ..., y_n) = \sum_{j \in \{0, 1, ..., n^2\}} \alpha_j \times P_j
\end{aligned}
$$


The probability of being in a state is based on the thermodynamics of transcription factor binding. When only one transcription factor is bound in a state:
$$
\begin{aligned}
P_j \propto \nu = \left(\frac{y}{k}\right)^{n}
\end{aligned}
$$


where the hill coefficient $n$ represents the cooperativity of binding and $k$ the transcription factor concentration at half-maximal binding. When multiple regulators are bound:
$$
\begin{aligned}
P_j \propto \nu =  \rho \times \prod_j \left(\frac{y_j}{k_j}\right)^{n_j}
\end{aligned}
$$


where $\rho$ represents the cooperativity of binding between the different transcription factors. 


$P_i$ is only proportional to $\nu$ because $\nu$ is normalized such that $\sum_{i} P_i = 1$.


To each differential equation, we added an additional stochastic term: 
$$
\begin{aligned}
\frac{dx_i}{dt} = m \times f(y_1, y_2, ...) - \lambda \times x_i &+ \eta \times \sqrt{x_i} \times \Delta W_t \\
\frac{dy_i}{dt} = r \times x_i - \Lambda \times y_i &+ \eta \times \sqrt{y_i} \times \Delta W_t
\end{aligned}
$$


with $\Delta W_t \sim \mathcal{N}(0, h)$. 


Similar to [@schaffter_genenetweaversilicobenchmark_2011], we sample the different parameters from random distributions, given in `r ref("stable", "samplers")`.


```{r}
#add_stable(
#  read_rds(figure_file(
#    "samplers.rds",
#    "2-dataset_characterisation/1-synthetic")
#  )[[params$table_format]], 
#  "samplers", 
#  "Distributions from which each parameter in the thermodynamic model was sampled."
#)
```


We converted each ODE to an SDE by adding a chemical Langevin equation, as described in [@schaffter_genenetweaversilicobenchmark_2011]. These SDEs were simulated using the Euler–Maruyama approximation, with time-step $h = 0.01$ and noise strength $\eta = 8$. The total simulation time varied between 5 for linear and bifurcating datasets, 10 for consecutive bifurcating, trifurcating and converging datasets, 15 for bifurcating converging datasets and 30 for linear long, cycle and bifurcating loop datasets. The burn-in period was for each simulation 2. Each network was simulated 32 times.


#### Simulation of the single-cell RNA-seq experiment


For each dataset we sampled the same number of cells as were present in the reference real dataset, limited to the simulation steps after burn-in. →These cells were sampled uniformly across the different steps of the 32 simulations. ← Next, we used the Splatter package [@zappia_splattersimulationsinglecell_2017] to estimate the different characteristics of a real dataset, such as the distributions of average gene expression, library sizes and dropout probabilities. We used Splatter to simulate the expression levels $\lambda_{i,j}$ of housekeeping genes $i$ (to match the number of genes in the reference dataset) in every cell $j$. These were combined with the expression levels of the genes simulated within a trajectory. Next, true counts were simulated using $Y'_{i,j} \sim \text{Poisson}(\lambda_{i,j})$. Finally, we simulated dropouts by setting true counts to zero by sampling from a Bernoulli distribution using a dropout probability $\pi^D_{i,j} =\frac{1}{1+e^{-k(\text{ln}(\lambda_{i,j})-x_0)}}$. →Both $x_0$ (the midpoint for the dropout logistic function) and $k$ (the shape of the dropout logistic function) were estimated by Splatter.←


This count matrix was then filtered and normalised using the pipeline described below.


#### Gold standard extraction


Because each cellular simulation follows the trajectory at its own speed, knowing the exact position of a cell within the trajectory topology is not straightforward. Furthermore, the speed at  which simulated cells make a decision between two or more alternative paths is highly variable. →We therefore first constructed a backbone expression profile for each branch within the trajectory. To do this, we first defined in which order the expression of the modules is expected to change, and then generated a backbone expression profile in which the expression of these modules increases and decreases smoothly between 0 and 1.  We also smoothed the expression in each simulation using a rolling mean with a window of 50 time steps, and then calculated the average module expression along the simulation. ← We used dynamic time warping, implemented in the dtw R package [@giorgino_computingvisualizingdynamic_2009; @tormene_matchingincompletetime_2009], with an open end to align a simulation to all possible module progressions, and then picked the alignment which minimised the normalised distance between the simulation and the backbone. In case of cyclical trajectory topologies, the number of possible milestones a backbone could progress through was limited to 20.


→
### dyntoy
```{r}
design_dyntoy <- read_rds(result_file("design_dyntoy.rds", "01-datasets/02-synthetic"))
```


For more simplistic data generation ("toy" datasets), we created the dyntoy workflow (`r print_url("https://github.com/dynverse/dyntoy")`) . We created `r length(unique(design_dyntoy$topology_model))` topology generators (described below), and with   `r nrow(design_dyntoy)/length(unique(design_dyntoy$topology_model))` datasets per generator, we created `r nrow(design_dyntoy)` datasets.


We created a set of topology generators:


* Linear and cyclic, with number of milestones $\sim B(10, 0.25)$
* Bifurcating and converging, with four milestones
* Binary tree, with number of branching points $\sim U(3, 6)$
* Tree, with number of branching points $\sim U(3, 6)$ and maximal degree $\sim U(3, 6)$


For more complex topologies we first calculated a random number of "modifications" $\sim U(3, 6)$ and a $\textit{deg}_{\textit{max}} \sim B(10, 0.25) + 1$. For each type of topology, we defined what kind of modifications are possible: divergences, loops, convergences and divergence-convergence. We then iteratively constructed the topology by uniformly sampling from the set of possible modifications, and adding this modification to the existing topology. For a divergence, we connected an existing milestone to a number of a new milestones. Conversely, for a convergence we connected a number of new nodes to an existing node. For a loop, we connected two existing milestones with a number of milestones in between. Finally for a divergence-convergence we connected an existing milestone to several new milestones which again converged on a new milestone. The number of nodes was sampled from $\sim B(\textit{deg}_{\textit{max}} - 3, 0.25) + 2$


* Diverging-converging, allowed divergence and converging modifications
* Diverging with loops, allowed divergence and loop modifications
* Multiple looping, allowed looping modifications
* Connected, allowed looping, divergence and convergence modifications
* Disconnected, number of components sampled from $\sim B(5, 0.25) + 2$, for each component we randomly chose a topology from the ones listed above


After generating the topology, we sampled the length of each edge $\sim U(0.5, 1)$. We added regions of delayed commitment to a divergence by virtually flipping a coin. [a]We then placed the number of cells (same number as from the reference real dataset), on this topology uniformly, depending on the length of the edges in the milestone network.[b]


For each gene (same number as from the reference real dataset), we calculated the Kamada-Kawai layout in 2 dimensions, with edge weight equal to the length of the edge. For this gene, we then extracted for each cell a density value using a bivariate normal distribution with $\mu \sim U(x_{\textit{min}}, x_{\textit{min}})$ and $\sigma \sim U(x_{\textit{min}}/10, x_{\textit{min}}/8)$. We used this density as input for a zero-inflated negative binomial distribution with $\mu ~ U(100, 1000) \times \textit{density}$, $k ~ U(\mu / 10, \mu / 4)$ and $pi$ from the parameters of the reference real dataset, to get the final count values.


This count matrix was then filtered and normalised using the pipeline described below.
### PROSSTT
```{r}
design_prosstt <- read_rds(result_file("design_dyntoy.rds", "01-datasets/02-synthetic"))
```


PROSSTT is a recent data simulator [@papadopoulosPROSSTTProbabilisticSimulation2018], which simulates expression using linear mixtures of expression programs and random walks through the trajectory. We used 5 topology generators from dyntoy (linear, bifurcating, multifurcating, binary tree and tree), and simulated for each topology generator 10 datasets using different reference real datasets, leading to a total of 50 datasets.


Using the `simulate_lineage` function, we simulated the lineage expression, with parameters $\a \sim U(0.01, 0.1)$, $\textit{branch-tol}_{\textit{intra}} \sim U(0, 0.9)$ and $\textit{branch-tol}_{\textit{inter}} \sim U(0, 0.9)$. These parameter distributions were chosen very broad so as to make sure both easy and difficult datasets are simulated. After simulating base gene expression with `simulate_base_gene_exp`, we used the `sample_density` function to finally simulate expression values of a number of cells (the same as from the reference real dataset), with $\alpha \sim \textit{Lognormal}$ ($\mu = 0.3$ and $\sigma = 1.5$) and $\beta \sim \textit{Lognormal}$ ($\mu = 2$ and $\sigma = 1.5$). Each of these parameters were centered around the default values of PROSSTT, but with enough variability to ensure a varied set of datasets.


This count matrix was then filtered and normalised using the pipeline described below.
### Splatter
Splatter [@zappia_splattersimulationsinglecell_2017] simulates expression values by constructing non-linear paths between different states, each having a distinct expression profile. We used 5 topology generators from dyntoy (linear, bifurcating, multifurcating, binary tree and tree), and simulated for each topology generator 10 datasets using different reference real datasets, leading to a total of 50 datasets.


We used the `splatSimulatePaths` function from Splatter to simulate datasets, with number of cells and genes equal to those in the reference real dataset, and with parameters  $\textit{nonlinearProb}$, $\textit{sigmaFac}$ and $\textit{skew}$ all sampled from $U(0, 1)$. 


This count matrix was then filtered and normalised using the pipeline described below.


←
## Dataset filtering and normalisation
We used a standard single-cell RNA-seq preprocessing pipeline which applies parts of the scran and scater Bioconductor packages [@lun_stepbystepworkflowlowlevel_2016]. The advantages of this pipeline is that it works both with and without spike-ins, and includes a harsh cell filtering which looks at abnormalities in library sizes, mitochondrial gene expression, and number of genes expressed using median absolute deviations (→which we set to 3←). We required that a gene was expressed in at least 5% of the cells, and that it should have an average expression higher than 0.02. Furthermore, we used the pipeline to select the most highly variable genes, using a false discovery rate of 5% and a biological component higher than 0.5. As a final filter, we removed both all-zero genes and cells until convergence.
## Evaluation metrics
The importance of using multiple metrics to compare complex models has been stated repeatedly [@norel_selfassessmenttrap_2011]. →Furthermore, a trajectory is a model with multiple layers of complexity, which calls for several metrics each assessing a different layer. We therefore defined several possible metrics for comparing trajectories, each investigating different layers. These are all discussed in `r ref("snote", "metrics")` along with examples and robustness analyses when appropriate.


Next, we created a set of rules to which we think a good trajectory metric should conform, and tested this empirically for each metric by comparing scores before and after perturbing a dataset (`r ref("snote", "metrics")`). Based on this analysis, we chose 4 metrics for the evaluation, each assessing a different aspect of the trajectory: (i) the `r label_metric("him")` measures the topological similarity, (ii) the  `r label_metric("F1_branches")` compares the branch assignment, (iii) the  `r label_metric("correlation")` assesses the similarity in pairwise cell-cell distances and thus the cellular positions, and (iv) the  `r label_metric("featureimp_cor")` looks at whether similar important features (genes) are found in both the reference dataset and the prediction. ←
### Correlation between geodesic distances
The similarity in cell ordering between two trajectories is assessed by calculating the geodesic distances between each pair of cells for both trajectories. The definition of a geodesic distance between two cells part of the common trajectory model will be demonstrated using a toy example (`r ref("sfig", "metric_geodesic")`). 


```{r}
add_sfig(
  ggplot(),#figure_file("geodesic.svg", experiment_id="manual_figures/metrics"), 
  "metric_geodesic", 
  "The modified geodesic distances demonstrated on a toy example.", 
  "a) A toy example containing four milestones (W to Z) and five cells (a to e). b) The corresponding milestone network, milestone percentages and regions of delayed commitment, when the toy trajectory is converted to the common trajectory model. c) The calculations made for calculating the pairwise geodesic distances. d) A heatmap representation of the pairwise geodesic distances."
)
```


The geodesic distance between two cells depends on whether they are (i) on the same transition, or (ii) in different transitions. In the first case, the distance is defined as the product of the difference in milestone percentages and the length of the transition they both reside on. For cells $a$ and $b$ in the example, $d(a, b)$ is equal to $1 \times (0.9 - 0.2) = 0.7$. In the latter case, the distances between the cells and all of their neighbouring milestones will be calculated. These distances in combination with the milestone network are used to calculate the shortest path distance between the two cells. For cells $a$ and $c$ in the example, $d(a, X) = 1 \times 0.9$ and $d(c, X) = 3 \times 0.2$, and therefore $d(a, c) = 1 \times 0.9 + 3 \times 0.2$. 


According to the defined common trajectory model (`r ref("fig", "ti_evaluation_overview", "b")`), cells are also allowed to have a delayed commitment. In a region of delayed commitment, one milestone will be connected to all other milestones as per the milestone network. In this case, the distance between two cells both inside a region of delayed commitment is calculated as the manhattan distances between the milestone percentages weighted by the transition weights from the milestone network. For cells $d$ and $e$ in the example, $d(d, e)$ is equal to $0 \times (0.3 - 0.2) + 2 \times (0.7 - 0.2) + 3 \times(0.4 - 0.1)$, which is equal to $1.9$. The distance between two cells where one is part of a region of delayed commitment is calculated similarly to the previous paragraph, by first calculating the distance between the cells and their neighbouring milestones first, then calculating the shortest path distances between the two.


Finally, calculating all pairwise distances between cells would scale poorly for trajectories with large numbers of cells. For this reason, a set of waypoint cells are defined *a priori*, and only the distances between the waypoint cells and all other cells is calculated, in order to calculate the correlation of geodesic distances of two trajectories. The waypoints are determined by viewing each milestone, transition and region of delayed commitment as a collection of cells, and sampling cells from the different collections weighted by the total number of cells within that collection. For calculating the correlation of geodesic distances between two trajectories, the distances between all cells and the union of both waypoint sets is computed. For the benchmark evaluation, the total number of waypoints sampled from a trajectory was one hundred.


### Random forest prediction error


Although the correlation between geodesic distances directly assesses the position of the cells in the trajectory, a bad correlation does not directly imply that similar cells were not grouped together by the method, as illustrated in `r ref("sfig", "score_aspects", c("b", "c"))`. For example, certain methods will inevitably reach an incorrect ordering because they cannot handle the correct trajectory type, but these methods could still correctly place similar cells next to each other. We therefore also included a metric which looks at the local neighbourhood of each cell, and assesses whether this neighbourhood can accurately predict the position of this cell in the gold standard. We used a Random Forest regression, implemented in the *ranger* package [@wright_rangerfastimplementation_2017] to separately predict milestone percentages of each cell in the gold standard, using the milestone percentages of this cells in the prediction as features. We then used the out-of-bag mean-squared error on these percentages to score each method's capability of predicting the correct neighbourhood of each cell.


### Edge flip score


As a third independent score, we assessed the similarity between the milestone network topologies. We first simplified each network, by merging consecutive linear edges into one edge, and adding new milestones within self loops such that $A \rightarrow A$ would be converted $A \rightarrow B \rightarrow C \rightarrow D$, by adding an intermediate node to linear networks. Because we are interested in the overall similarity between two topologies irrespective of the direction of the edges, the network was made undirected. Next, we define the edge flip score as the minimal number of edges which should be added or removed to convert one network into the other, divided by the total number of edges in both networks. This problem is equivalent to the maximum common edge subgraph problem, a known NP-hard problem without a scalable solution [@bahiense_maximumcommonedge_2012]. We implemented a branch and bound approach for this problem, by first enumerating all possible edge additions and removals with the minimal number of edges (the edge difference between the two networks) and if none of the new networks was isomorphic, we tried out all solutions with additional two edge changes. To further limit the search space, we made sure the degree distributions between the two networks were  similar, before assessing whether the two networks were isomorphic using the BLISS algorithm [@junttila_engineeringefficientcanonical_2007], as implemented in the R *igraph* package.


A comparison of edge flip scores between common trajectory topologies is illustrated in `r ref("sfig", "edge_flip_comparison")`.


```{r}
add_sfig(
  ggplot(),#figure_file("edge_flip_comparison.svg", experiment_id="3-metric_characterisation/4-edge_flip"), 
  "edge_flip_comparison", 
 "Edge flip score values for common trajectory topologies",
  ""
)
```
### Aggregation of scores
`r ref("sfig", "aggregation_of_results")` illustrates the full set of aggregations performed on the raw scores in order to arrive at the final ranking of methods. 


**Step 1:** In order to make the scores produced by the different metrics comparable to one another, across datasets of varying difficulty, the raw scores are transformed per metric per dataset to a $[0,1]$ range as follows: 


<!--
$$
\begin{aligned}
\text{scale}(x) &= \text{sigmoid}\left( \frac{x - \textrm{mean}(x)}{\textrm{max}(\textrm{abs}(x - \textrm{mean}(x))) \times 5} \right)
\end{aligned}
$$
-->


$$
\begin{aligned}
\text{scale}(x) &= \text{sigmoid}\left( \frac{x - \bar{x}}{\textrm{max}(\textrm{abs}(x - \bar{x})) \times 5} \right)\text{, where } \bar{x} \text{ is the arithmetic mean of } x.
\end{aligned}
$$


Here, the mean is subtracted from the raw score in order to center the distribution around zero. The denominator is used to scale the range of the distribution such that, after sigmoid transformation, the minimum and maximum transformed scores lie close to 0 and/or 1, respectively. 


**Step 2:** After transformation, the arithmetic mean is calculated across replicates.


**Step 3:** The normalised scores is combined across all methods and all datasets.


**Step 4:** In order to remove effects from unbalanced trajectory types (e.g. an overrepresentation of linear datasets), the arithmetic mean is first computed per trajectory type.


**Step 5:** An overall score is computed by calculating the arithmetic mean across all trajectory types.


**Step 6:** A ranking of the methods is obtained by calculating the harmonic mean between the different metrics, thus requiring a method to obtain a high score on all three metrics in order to obtain a high overall score.




```{r}
add_sfig(
  raw_file("overview_aggregation_v1.svg", "08-summary"), 
  "aggregation_of_results", 
  "Aggregation methodology of metric scores.",
  "",
  15, 
  5
)
```
## Benchmark
### Method execution
Each execution of a method on a dataset was performed in a separate task as part of a gridengine job. Each task was allocated one CPU core of an Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz, and one R session was started for each task. During the execution of a method on a dataset, if the time limit (>6h) or memory limit (32GB) was exceeded, or an error was produced, a zero score was returned for that execution. If a method consistently generated errors on this dataset across all replicates, the error is called “data-specific”, otherwise “stochastic”. The environment variable `R_MAX_NUM_DLLS` set to 500. The `base::set.seed` was overridden in order to prevent stochastic TI methods from pretending to be deterministic and/or robust. Timings of methods were measured for different steps along the executions, including preprocessing, postprocessing, each of the different metrics, and the method itself.
### Effect of prior information
To assess the effect of prior information on the performance of a method, we compared the performance of methods which do or do not require the prior information using a two-tailed Mann–Whitney U test. P-values were controlled for multiple testing using Benjamini-Hochberg correction.
### Effect of algorithm components
To assess the effect of algorithm components on the performance of a method, we used (1) a two-tailed Mann–Whitney U test, as implemented in the `wilcox.test` R function, and (2) increase in node purity importance scores using random forest classification, as implemented in the R randomForest package, predicting whether a method scored better than the median score. To evaluate whether a method increased or decreased performance, we used the estimate of the location parameter of the Mann–Whitney U test. We only investigated algorithm components which were part of at least 4 different methods in our evaluation study. P-values were controlled for multiple testing using Benjamini-Hochberg correction.
## Method quality control


We created a transparent scoring scheme to check the quality of each method based on several existing tool quality and programming guidelines in literature and online (`r ref("stable", "qc_checks")`). The goal of this quality control in the first place is to stimulate the improvement of current methods, and the development of user and developer friendly new methods. The quality control assessed 6 categories, each looking at several aspects, which are further divided into individual items. The availability category checks whether the method is easily available, whether the code and dependencies can be easily installed, and how the method can be used. The code quality assesses the quality of the code both from a user perspective (function naming, dummy proofing and availability of plotting functions) and a developer perspective (consistent style and code duplication). The code assurance category is frequently overlooked, and checks for code testing, continuous integration [@beaulieu-jones_reproducibility_2017] and an active support system. The documentation category checks the quality of the documentation, both externally (tutorials and function documentation) and internally (inline documentation). The behaviour category assesses the ease by which the method can be run, by looking for unexpected output files and messages, prior information and how easy the trajectory model can be extracted from the output. Finally, we also assessed certain aspects of the study in which the method was proposed, such as publication in a peer-reviewed journal, the number of dataset in which the usefulness of the method was shown, and the scope of method evaluation in the paper.


```{r}
#add_stable(
#  read_rds(figure_file(
#    "qc_checks.rds",
#    "4-method_characterisation"
#  ))[[params$table_format]], 
#  "qc_checks", 
#  "Scoring scheme for method quality control. Each quality aspect was given a weight based on how many times it was mentioned in a set of articles discussing best practices for tool development."
#)
```


Each aspect was further assigned to one or more applications, based on whether it influenced the user friendliness of the tool (such as code availability, good documentation or contains plotting functions), the developer friendliness of the tool (such as unit testing, inline documentation and a clear licensing), or indications that the tool will be broadly applicable on new datasets (such as being open-source, containing a good tutorial and support system, and being thoroughly evaluated in the study where it was published).


Each quality aspect received a weight depending on how frequently it was found in several papers and online sources which discuss tool quality (`r ref("stable", "qc_checks")`). This was to make sure that more important aspects, such as the open source availability of the method, outweighed other aspects, such as the availability of a graphical user interface. Within each aspect, we also assigned a weight to the individual questions being investigated (`r ref("stable", "qc_checks")`). For calculating the final score, we weighed each of the six categories equally.
→
## Scalability
←
## Trajectory types
We classified all possible trajectory topologies into distinct trajectory types, based on topological criteria (`r ref("fig", "ti_evaluation_overview", "c")`). These trajectory types start from the most general trajectory type, a disconnected directed graph, and move down (within a directed acyclic graph structure), progressively becoming more simple until the two basic types: linear and cyclical. A disconnected directed graph is a graph in which only one edge can exist between two nodes. A directed graph is a disconnected graph in which all nodes are (indirectly) connected. A directed acyclic graph is a graph containing no cycles. A rooted tree is a directed acyclic graph containing no convergences (no nodes with in-degree higher than 1). A rooted binary tree is a rooted tree with every node having maximally two outgoing edges. A convergence is a directed acyclic graph in which only one node has a degree larger than one and this same node has an indegree of one. A multifurcation is a rooted tree in which only one node has a degree larger than one. A bifurcation is a multifurcation in which only one node has a degree equal to 3. A directed linear graph is a graph in which no node has a degree larger than 3. Finally, a directed cycle is a directed graph in which no node has a degree larger than 3 but contains one cycle. For every directed trajectory type, a corresponding undirected trajectory type can also be defined, with directed acyclic graphs and directed graphs merging to undirected graph, and convergence and bifurcation merging to a simple fork. In most cases, a method which was able to detect a complex trajectory type, was also able to detect less complex trajectory types, with →the exceptions being being DPT (limited to bifurcating trajectories), and STEMNET and FateID (unable to return linear trajectories).←




```{r "supplementary", echo=FALSE, results='asis'}
#if (params$table_format == "html") 
#  source(paste0(dynalysis::get_dynalysis_folder(), "/analysis/paper/sfigs_stables.R"))
#write_rds(lst(refs, sfigs, stables), derived_file("sfigs_stables.rds", "paper"))
#write_rds(figs, derived_file("figs.rds", "paper"))
```












<!--
Tips for working in this document:
Go to tools -> preferences and remove Use smart quotes
Enable automatic zotero syncing to bibtex
Make sure to pin the zotero bibtex references after first using them!
-->


[a]Don't put this in the paper
[b]Factually incorrect but ¯\_(ツ)_/¯